{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: cannot import name randbits",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig_parameters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MARKET_DATA_DICT\n",
      "File \u001b[1;32mc:\\Users\\Arek\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _missing_dependencies:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_missing_dependencies)\n\u001b[0;32m     34\u001b[0m     )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: cannot import name randbits"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from config_parameters import MARKET_DATA_DICT\n",
    "from secrets_template import API_PROFIT_KEY\n",
    "from datetime import datetime\n",
    "\n",
    "your_api_key = API_PROFIT_KEY\n",
    "folder_data = MARKET_DATA_DICT\n",
    "\n",
    "# Functions to check available datas:\n",
    "\n",
    "def get_stock_exchanges(api_key):\n",
    "    url = \"https://api.profit.com/data-api/reference/exchanges\"\n",
    "    params = {\n",
    "        \"token\": api_key,\n",
    "        \"type\": \"stocks\"\n",
    "        #,\"code\": \"US\",\n",
    "        #\"name\": \"USA Stocks\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "    \n",
    "def save_data_to_file(data, folder=\"Profit2.com\", filename=\"stocks.json\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Data saved to {filepath}\")\n",
    "\n",
    "\n",
    "def load_data_from_file(folder=\"Profit2.com\", filename=\"stocks.json\"):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File {filepath} does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    with open(filepath, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    print(f\"Data loaded from {filepath}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_indices_data(api_key):\n",
    "    url = \"https://api.profit.com/data-api/reference/indices\"\n",
    "    params = {\n",
    "        \"token\": api_key,\n",
    "        \"skip\": 0,\n",
    "        \"limit\": 1000,\n",
    "        \"exchange\": \"F\",\n",
    "        \"available_data\": \"historical\",\n",
    "        \"type\": \"INDEX\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "def get_stock_data(api_key):\n",
    "    url = \"https://api.profit.com/data-api/reference/stocks\"\n",
    "    params = {\n",
    "        \"token\": api_key,\n",
    "        \"skip\": 0,\n",
    "        \"limit\": 1000,\n",
    "        #\"symbol\": \"TSLA\",\n",
    "        \"exchange\": \"NASDAQ\",\n",
    "        \"country\": \"United States\",\n",
    "        \"currency\": \"USD\",\n",
    "        \"available_data\": \"historical\",\n",
    "        \"type\": \"Common Stock\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "def get_forex_data(api_key):\n",
    "    url = \"https://api.profit.com/data-api/reference/forex\"\n",
    "    params = {\n",
    "        \"token\": api_key,\n",
    "        #\"group\": \"MAJOR\",\n",
    "        #\"currency_base\": \"EUR\",\n",
    "       # \"currency_quote\": \"USD\",\n",
    "        \"skip\": 0,\n",
    "        \"limit\": 1000,\n",
    "        \"available_data\": \"historical\" #, fundamential\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "    \n",
    "def get_commodity_data(api_key):\n",
    "    url = \"https://api.profit.com/data-api/reference/commodity\"\n",
    "    params = {\n",
    "        \"token\": api_key,\n",
    "       # \"group\": \"ENERGY\",\n",
    "        \"skip\": 0,\n",
    "        \"limit\": 1000,\n",
    "        \"available_data\": \"historical\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "def get_bonds_data(api_key):\n",
    "    url = \"https://api.profit.com/data-api/reference/bonds\"\n",
    "    params = {\n",
    "        \"token\": api_key,\n",
    "        \"skip\": 0,\n",
    "        \"limit\": 1000,\n",
    "        #\"symbol\": \"US10Y\",\n",
    "        \"available_data\": \"historical\",\n",
    "        \"country\": \"United States\",\n",
    "        \"currency\": \"USD\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def save_data_to_file(data, folder=\"Profit.com\", filename=\"stocks.json\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Data saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching for available data, for example:\n",
    "    \n",
    "# data_forex = get_forex_data(your_api_key)\n",
    "\n",
    "# if data_forex:\n",
    "#     save_data_to_file(data_forex, filename=\"forex_all.json\")\n",
    "\n",
    "\n",
    "# data_stocks = get_stock_exchanges(your_api_key)\n",
    "\n",
    "# data_indices = get_indices_data(your_api_key)\n",
    "\n",
    "# data_tsla = get_stock_data(your_api_key)\n",
    "\n",
    "# data_forex = get_forex_data(your_api_key)\n",
    "\n",
    "# data_crypto = get_crypto_data(your_api_key)\n",
    "\n",
    "# data_commodity = get_commodity_data(your_api_key)\n",
    "\n",
    "# data_bonds = get_bonds_data(your_api_key)\n",
    "\n",
    "# if data_stocks:\n",
    "#     save_data_to_file(data_stocks, filename=\"stocks.json\")\n",
    "\n",
    "# if data_indices:\n",
    "#     save_data_to_file(data_indices, filename=\"indices.json\")\n",
    "\n",
    "# if data_tsla:\n",
    "#     save_data_to_file(data_tsla, filename=\"tsla.json\")\n",
    "\n",
    "# if data_forex:\n",
    "#     save_data_to_file(data_forex, filename=\"forex.json\")\n",
    "\n",
    "# if data_crypto:\n",
    "#     save_data_to_file(data_crypto, filename=\"crypto.json\")\n",
    "\n",
    "# if data_commodity:\n",
    "#     save_data_to_file(data_commodity, filename=\"commodity.json\")\n",
    "\n",
    "# if data_bonds:\n",
    "#     save_data_to_file(data_bonds, filename=\"bonds.json\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading daily data:\n",
    "def get_historical_stock_data(api_key, ticker):\n",
    "    url = f\"https://api.profit.com/data-api/market-data/historical/daily/{ticker}\"\n",
    "    params = {\n",
    "        \"token\": api_key,\n",
    "        \"ticker\":ticker,\n",
    "        \"start_date\": \"1990-01-01\",\n",
    "        \"end_date\": \"2025-02-10\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Downloading intrady data ( interval 1m)\n",
    "def get_intraday_stock_data(api_key, ticker):\n",
    "    url = f\"https://api.profit.com/data-api/market-data/historical/intraday/{ticker}\"\n",
    "    params = {\n",
    "        \"token\": api_key,\n",
    "        \"start_time\": 1736467200,  # Timestamp for 2025-01-10\n",
    "        \"end_time\": 1738281600,    # Timestamp for 2025-01-31\n",
    "        \"interval\": \"1m\",\n",
    "        \"limit\": 500\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exapmle of downloading data:\n",
    "\n",
    "# ticker=\"DJI.INDX\"\n",
    "# data_historical_tsla = get_intraday_stock_data(your_api_key, ticker)\n",
    "\n",
    "# print(data_historical_tsla)\n",
    "# if data_historical_tsla:\n",
    "#     save_data_to_file(data_historical_tsla, filename=f\"{ticker}_intervals.json\")\n",
    "#     print(len(data_historical_tsla))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading intraday data for custom interval, from first available till specific end\n",
    "def get_intraday_stock_data_batch(api_key, ticker, interval, end_time=1738368000, batches=10000, batch_size=500):\n",
    "    \"\"\"\n",
    "    Fetches stock data in specifiv interval intervals, starting from the given end time\n",
    "    and going backwards in batches of batch_size.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.profit.com/data-api/market-data/historical/intraday/{ticker}\"\n",
    "    \n",
    "    all_data = []\n",
    "    i = 0\n",
    "    while i < batches:  \n",
    "        params = {\n",
    "            \"token\": api_key,\n",
    "            \"end_time\": end_time,  # Timestamp for ending time\n",
    "            \"interval\": interval,\n",
    "            \"limit\": batch_size\n",
    "        }\n",
    "        response = requests.get(url, params=params, stream=True)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data:\n",
    "                break  # Break if no data is available\n",
    "            \n",
    "            all_data = data + all_data\n",
    "\n",
    "            # Set the new end_time to the oldest timestamp from the fetched data\n",
    "            end_time = min(item['t'] for item in data)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "        i += 1\n",
    "    # Convert to DataFrame\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df['timestamp'] = pd.to_datetime(df['t'], unit='s')\n",
    "        df.rename(columns={'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}, inplace=True)\n",
    "        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_resampled_data(folder, df, ticker, interval):\n",
    "    \n",
    "\n",
    "    symbol = ticker.replace('/', '.')\n",
    "    # Create a path for the file\n",
    "    symbol_directory = os.path.join(folder, symbol)\n",
    "    if not os.path.exists(symbol_directory):\n",
    "        # If it doesn't exist, create it\n",
    "        os.makedirs(symbol_directory)\n",
    "    file_path = os.path.join(symbol_directory, interval + '.csv')\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def resample_data(df, interval):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Set the 'timestamp' column as the index\n",
    "    new_df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    if interval.endswith('m'):\n",
    "        interval = interval[:-1] + 't'\n",
    "    new_df.set_index('timestamp', inplace=True)\n",
    "    resampled_df = new_df.resample(interval).agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }).dropna()\n",
    "    resampled_df.reset_index(inplace=True)\n",
    "    return resampled_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling daily data to higher time intervals\n",
    "def resample_daily_to_weekly(df, interval, reference_date=None):\n",
    "    # Convert 'timestamp' column to datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Set 'timestamp' as index\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    first_entry = df.iloc[0]\n",
    "\n",
    "    # Resample to weekly data starting from Monday\n",
    "    if interval == '1w':\n",
    "        df_resampled = df.resample('W-MON', label='left', closed='left').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "    elif interval == '2w':\n",
    "        df_resampled = df.resample('2W-MON', label='left', closed='left').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "        # Filter resampled data around the reference date (±12 days)\n",
    "        closest_row = df_resampled[(df_resampled.index >= reference_date - pd.Timedelta(days=12)) & \n",
    "                                   (df_resampled.index <= reference_date + pd.Timedelta(days=12))]\n",
    "\n",
    "        # If a close match is found, calculate how many days are missing\n",
    "        if not closest_row.empty:\n",
    "            closest_match = closest_row.index[abs((closest_row.index - reference_date).days).argmin()]\n",
    "            closest_diff = abs((closest_match - reference_date).days)\n",
    "        else:\n",
    "            closest_diff = 0\n",
    "\n",
    "        # If reference alignment needed, pad data by inserting dummy rows\n",
    "        if closest_diff > 0:\n",
    "            df_new = df.copy()\n",
    "            for i in range(1, closest_diff + 1):\n",
    "                new_index = df_new.index[0] - pd.Timedelta(days=1)\n",
    "                new_entry = pd.DataFrame({\n",
    "                    'open': [first_entry.open],\n",
    "                    'high': [first_entry.open],\n",
    "                    'low': [first_entry.open],\n",
    "                    'close': [first_entry.open],\n",
    "                    'volume': [0]\n",
    "                }, index=[new_index])\n",
    "                df_new = pd.concat([new_entry, df_new]).sort_index()\n",
    "\n",
    "            df_resampled = df_new.resample('2W-MON', label='left', closed='left').agg({\n",
    "                'open': 'first',\n",
    "                'high': 'max',\n",
    "                'low': 'min',\n",
    "                'close': 'last',\n",
    "                'volume': 'sum'\n",
    "            }).dropna()\n",
    "\n",
    "    elif interval == '1M':\n",
    "        df_resampled = df.resample('MS', label='left', closed='left').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "    elif interval == '2M':\n",
    "        # Take every second month and resample from month start\n",
    "        df_resampled = df[df.index.month % 2 == 0].resample('MS').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "    elif interval == '2d':\n",
    "        df_resampled = df.resample('2D', label='left', closed='left').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "        # Align with reference date ±1 day\n",
    "        closest_row = df_resampled[(df_resampled.index >= reference_date - pd.Timedelta(days=1)) & \n",
    "                                   (df_resampled.index <= reference_date + pd.Timedelta(days=1))]\n",
    "\n",
    "        if not closest_row.empty:\n",
    "            closest_match = closest_row.index[abs((closest_row.index - reference_date).days).argmin()]\n",
    "            closest_diff = abs((closest_match - reference_date).days)\n",
    "        else:\n",
    "            closest_diff=0\n",
    "\n",
    "        if closest_diff > 0:\n",
    "            df_new = df.copy()\n",
    "            for i in range(1, closest_diff + 1):\n",
    "                new_index = df_new.index[0] - pd.Timedelta(days=1)\n",
    "                new_entry = pd.DataFrame({\n",
    "                    'open': [first_entry.open],\n",
    "                    'high': [first_entry.open],\n",
    "                    'low': [first_entry.open],\n",
    "                    'close': [first_entry.open],\n",
    "                    'volume': [0]\n",
    "                }, index=[new_index])\n",
    "                df_new = pd.concat([new_entry, df_new]).sort_index()\n",
    "\n",
    "            df_resampled = df_new.resample('2D', label='left', closed='left').agg({\n",
    "                'open': 'first',\n",
    "                'high': 'max',\n",
    "                'low': 'min',\n",
    "                'close': 'last',\n",
    "                'volume': 'sum'\n",
    "            }).dropna()\n",
    "\n",
    "    elif interval == '3d':\n",
    "        df_resampled = df.resample('3D', label='left', closed='left').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "        # Align with reference date ±2 days\n",
    "        closest_row = df_resampled[(df_resampled.index >= reference_date - pd.Timedelta(days=2)) & \n",
    "                                   (df_resampled.index <= reference_date + pd.Timedelta(days=2))]\n",
    "\n",
    "        if not closest_row.empty:\n",
    "            closest_match = closest_row.index[abs((closest_row.index - reference_date).days).argmin()]\n",
    "            closest_diff = abs((closest_match - reference_date).days)\n",
    "        else:\n",
    "            closest_diff=0\n",
    "        if closest_diff > 0:\n",
    "            df_new = df.copy()\n",
    "            for i in range(0, closest_diff):\n",
    "                new_index = df_new.index[0] - pd.Timedelta(days=1)\n",
    "                new_entry = pd.DataFrame({\n",
    "                    'open': [first_entry.open],\n",
    "                    'high': [first_entry.open],\n",
    "                    'low': [first_entry.open],\n",
    "                    'close': [first_entry.open],\n",
    "                    'volume': [0]\n",
    "                }, index=[new_index])\n",
    "                df_new = pd.concat([new_entry, df_new]).sort_index()\n",
    "\n",
    "            df_resampled = df_new.resample('3D', label='left', closed='left').agg({\n",
    "                'open': 'first',\n",
    "                'high': 'max',\n",
    "                'low': 'min',\n",
    "                'close': 'last',\n",
    "                'volume': 'sum'\n",
    "            }).dropna()\n",
    "\n",
    "    elif interval == '3M':\n",
    "        df_resampled = df.resample('QS', label='left', closed='left').agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }).dropna()\n",
    "\n",
    "        # Align with reference date ±60 days\n",
    "        closest_row = df_resampled[(df_resampled.index >= reference_date - pd.Timedelta(days=60)) & \n",
    "                                   (df_resampled.index <= reference_date + pd.Timedelta(days=60))]\n",
    "\n",
    "        if not closest_row.empty:\n",
    "            closest_match = closest_row.index[abs((closest_row.index - reference_date).days).argmin()]\n",
    "            closest_diff = abs((closest_match - reference_date).days)\n",
    "        else:\n",
    "            closest_diff=0\n",
    "        if closest_diff > 0:\n",
    "            df_new = df.copy()\n",
    "            for i in range(1, closest_diff + 1):\n",
    "                new_index = df_new.index[0] - pd.Timedelta(days=1)\n",
    "                new_entry = pd.DataFrame({\n",
    "                    'open': [first_entry.open],\n",
    "                    'high': [first_entry.open],\n",
    "                    'low': [first_entry.open],\n",
    "                    'close': [first_entry.open],\n",
    "                    'volume': [0]\n",
    "                }, index=[new_index])\n",
    "                df_new = pd.concat([new_entry, df_new]).sort_index()\n",
    "\n",
    "            df_resampled = df_new.resample('Q', label='left', closed='left').agg({\n",
    "                'open': 'first',\n",
    "                'high': 'max',\n",
    "                'low': 'min',\n",
    "                'close': 'last',\n",
    "                'volume': 'sum'\n",
    "            }).dropna()\n",
    "\n",
    "    # Reset index to make timestamp a column again\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "# Load data from CSV file for the given ticker and interval\n",
    "def load_data(ticker, interval, folder):\n",
    "    # Replace '/' with '.' in the ticker name\n",
    "    symbol = ticker.replace('/', '.')\n",
    "\n",
    "    # Build folder path\n",
    "    symbol_directory = os.path.join(folder, symbol)\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(symbol_directory):\n",
    "        os.makedirs(symbol_directory)\n",
    "\n",
    "    # Build full file path\n",
    "    file_path = os.path.join(symbol_directory, interval + '.csv')\n",
    "\n",
    "    # Load CSV if it exists\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Main function: load, resample, and save\n",
    "def resample_to_higher_and_save(folder, symbol, interval, reference_data):\n",
    "    basic_data = load_data(symbol, '1d', folder)\n",
    "    resampled_data = resample_daily_to_weekly(basic_data, interval, reference_date=reference_data)\n",
    "    save_resampled_data(folder_data, resampled_data, symbol, interval)\n",
    "\n",
    "def resample_data_and_save(basic_data, folder, symbol, intervals):   \n",
    "        for interval in intervals:\n",
    "            data_resampled = resample_data(basic_data, interval)\n",
    "            save_resampled_data(folder, data_resampled, symbol, interval)\n",
    "\n",
    "def save_row_data_5m_1d(ticker, start):\n",
    "    basic_data=get_intraday_stock_data_batch(your_api_key, ticker, \"5m\", start, 20000, 500)\n",
    "    save_resampled_data(folder_data, basic_data, ticker, \"5m\")\n",
    "    resample_data_and_save(basic_data, folder_data, ticker, [ '15m', '30m', '1h', '2h', '4h', '6h', '12h' ,'1d'])\n",
    "    \n",
    "def save_row_data_1m_1d(ticker, start):\n",
    "    basic_data=get_intraday_stock_data_batch(your_api_key, ticker, \"1m\", start, 20000, 500)\n",
    "    save_resampled_data(folder_data, basic_data, ticker, \"1m\")\n",
    "    resample_data_and_save(basic_data, folder_data, ticker, [ '5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h' ,'1d'])\n",
    "\n",
    "def save_row_data_15m_1d(ticker, start):\n",
    "    basic_data=get_intraday_stock_data_batch(your_api_key, ticker, \"15m\", start, 20000, 500)\n",
    "    save_resampled_data(folder_data, basic_data, ticker, \"15m\")\n",
    "    resample_data_and_save(basic_data, folder_data, ticker, [ '30m', '1h', '2h', '4h', '6h', '12h' ,'1d'])\n",
    "#save_row_data_1m_1d(\"NVDA\", \"15m\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_timestamp(year, month, day, hour=0, minute=0, second=0):\n",
    "    dt = datetime(year, month, day, hour, minute, second)\n",
    "    return int(dt.timestamp())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_profit_com_data\u001b[39m(tickers, min_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5m\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_ts\u001b[38;5;241m=\u001b[39mto_timestamp(\u001b[38;5;241m2025\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;66;03m# Download raw data depending on min_interval\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# if min_interval == \"1m\":\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m#     raise ValueError(f\"Unsupported min_interval: {min_interval}. Supported: '1m', '5m', '15m'\")\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         resample_to_higher_and_save(folder_data, ticker, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, reference_data\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mTimestamp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-01-31\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m, in \u001b[0;36mto_timestamp\u001b[1;34m(year, month, day, hour, minute, second)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_timestamp\u001b[39m(year, month, day, hour\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, minute\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, second\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     dt \u001b[38;5;241m=\u001b[39m datetime(year, month, day, hour, minute, second)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(dt\u001b[38;5;241m.\u001b[39mtimestamp())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "def download_profit_com_data(tickers, min_interval=\"5m\", start_ts=to_timestamp(2025, 3, 1)):\n",
    "\n",
    "\n",
    "\n",
    "    for ticker in tickers:\n",
    "            # Download raw data depending on min_interval\n",
    "        # if min_interval == \"1m\":\n",
    "        #     save_row_data_1m_1d(ticker, start_ts)\n",
    "        # elif min_interval == \"5m\":\n",
    "        #     save_row_data_5m_1d(ticker,  start_ts)\n",
    "        # elif min_interval == \"15m\":\n",
    "        #     save_row_data_15m_1d(ticker,  start_ts)\n",
    "        # else:\n",
    "        #     raise ValueError(f\"Unsupported min_interval: {min_interval}. Supported: '1m', '5m', '15m'\")\n",
    "        resample_to_higher_and_save(folder_data, ticker, \"2d\", reference_data=pd.Timestamp(\"2025-01-31\"))\n",
    "        resample_to_higher_and_save(folder_data, ticker, \"3d\", reference_data=pd.Timestamp(\"2025-01-30\"))\n",
    "        resample_to_higher_and_save(folder_data, ticker, \"1w\", reference_data=pd.Timestamp(\"2024-12-30\"))\n",
    "        resample_to_higher_and_save(folder_data, ticker, \"2w\", reference_data=pd.Timestamp(\"2024-12-30\"))\n",
    "        resample_to_higher_and_save(folder_data, ticker, \"1M\", reference_data=pd.Timestamp(\"2024-12-01\"))\n",
    "        resample_to_higher_and_save(folder_data, ticker, \"3M\", reference_data=pd.Timestamp(\"2024-10-01\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers= ['USDJPY.FOREX','NDX.INDX', 'DJI.INDX', 'NATGAS.OANDA' , 'XAUUSD.FOREX', 'PLTR', 'PG', 'USDJPY.PEPPERSTONE' , 'GSPC.INDX', 'USDJPY.PEPPERSTONE','XAGUSD.OANDA', 'XAUUSD.OANDA', 'XPDUSD.OANDA' , 'COFFEE.PEPPERSTONE' , 'COCOA.PEPPERSTONE', 'EURJPY.OANDA', 'EURUSD.OANDA', 'USDPLN.FOREX' ]\n",
    "tickers= ['USDJPY.FOREX','NDX.INDX', 'DJI.INDX', 'NATGAS.OANDA' , 'XAUUSD.FOREX']\n",
    "          \n",
    "download_profit_com_data(tickers, \"15m\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bot-trading)",
   "language": "python",
   "name": "bot-trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
